{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29755c4a",
   "metadata": {},
   "source": [
    "# Live text recognition with PaddleOCR via OpenVINO\n",
    "\n",
    "### PaddleOCR covered text detection and text recognition where text detection focuses on finding text area from input image, whereas text recognition converts obtained text into characters and words as an output.\n",
    "PaddleOCR pre-trained model to used in this refer to the â€œEnglish ultra-lightweight PP-OCRv3 model (13.4M)\" which can be found on GitHub: __[PP-OCR Series Model List](https://github.com/PaddlePaddle/PaddleOCR)__. In this notebook we'll try live text recognition with PaddleOCR via camera / video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd29b3",
   "metadata": {},
   "source": [
    "#### <i>1) Importing necessaried libraries and packages</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ba363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessaries libraries\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import paddle\n",
    "import math\n",
    "import time\n",
    "import collections\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "from openvino.runtime import Core\n",
    "from IPython import display\n",
    "import copy\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils\n",
    "import pre_post_processing as processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62aa605",
   "metadata": {},
   "source": [
    "#### <i>2) Defining function to download text detection and recognition pre-trained models from PaddleOCR resources</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to download text detection and recognition models from PaddleOCR resources\n",
    "\n",
    "def run_model_download(model_url, model_file_path):\n",
    "    \"\"\"\n",
    "    Download pre-trained models from PaddleOCR resources\n",
    "\n",
    "    Parameters:\n",
    "        model_url: url link to pre-trained models\n",
    "        model_file_path: file path to store the downloaded model\n",
    "    \"\"\"\n",
    "    model_name = model_url.split(\"/\")[-1]\n",
    "\n",
    "    if model_file_path.is_file():\n",
    "        print(\"Model already exists\")\n",
    "    else:\n",
    "        # Download the model from the server, and untar it.\n",
    "        print(\"Downloading the pre-trained model... May take a while...\")\n",
    "\n",
    "        # create a directory\n",
    "        os.makedirs(\"model\", exist_ok=True)\n",
    "        urllib.request.urlretrieve(model_url, f\"model/{model_name} \")\n",
    "        print(\"Model Downloaded\")\n",
    "\n",
    "        file = tarfile.open(f\"model/{model_name} \")\n",
    "        res = file.extractall(\"model\")\n",
    "        file.close()\n",
    "        if not res:\n",
    "            print(f\"Model Extracted to {model_file_path}.\")\n",
    "        else:\n",
    "            print(\"Error Extracting the model. Please check the network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb9d0a",
   "metadata": {},
   "source": [
    "#### <i>3) Defining directory path for downloading pre-trained text detection model</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4785c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where TEXT DETECTION model will be downloaded\n",
    "\n",
    "det_model_url = \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_infer.tar\"\n",
    "det_model_file_path = Path(\"model/ch_ppocr_mobile_v2.0_det_infer/inference.pdmodel\")\n",
    "\n",
    "run_model_download(det_model_url, det_model_file_path)\n",
    "\n",
    "# initialize inference engine for text detection\n",
    "core = Core()\n",
    "det_model = core.read_model(model=det_model_file_path)\n",
    "det_compiled_model = core.compile_model(model=det_model, device_name=\"CPU\")\n",
    "\n",
    "# get input and output nodes for text detection\n",
    "det_input_layer = det_compiled_model.input(0)\n",
    "det_output_layer = det_compiled_model.output(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f299616",
   "metadata": {},
   "source": [
    "#### <i>4) Defining directories path for downloading pre-trained text recognition model</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f932821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where TEXT RECOGNITION model will be downloaded\n",
    "\n",
    "rec_model_url = \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_infer.tar\"\n",
    "rec_model_file_path = Path(\"model/ch_ppocr_mobile_v2.0_rec_infer/inference.pdmodel\")\n",
    "\n",
    "run_model_download(rec_model_url, rec_model_file_path)\n",
    "\n",
    "# read the model and corresponding weights from file\n",
    "rec_model = core.read_model(model=rec_model_file_path)\n",
    "\n",
    "# assign dynamic shapes to every input layer on the last dimension\n",
    "for input_layer in rec_model.inputs:\n",
    "    input_shape = input_layer.partial_shape\n",
    "    input_shape[3] = -1\n",
    "    rec_model.reshape({input_layer: input_shape})\n",
    "\n",
    "rec_compiled_model = core.compile_model(model=rec_model, device_name=\"CPU\")\n",
    "\n",
    "# get input and output nodes\n",
    "rec_input_layer = rec_compiled_model.input(0)\n",
    "rec_output_layer = rec_compiled_model.output(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a004368",
   "metadata": {},
   "source": [
    "#### <i>5) Image preprocessing for text detection</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f240d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess for text detection\n",
    "def image_preprocess(input_image, size):\n",
    "    \"\"\"\n",
    "    Preprocess input image for text detection\n",
    "\n",
    "    Parameters:\n",
    "        input_image: input image\n",
    "        size: value for the image to be resized for text detection model\n",
    "    \"\"\"\n",
    "    img = cv2.resize(input_image, (size, size))\n",
    "    img = np.transpose(img, [2, 0, 1]) / 255\n",
    "    img = np.expand_dims(img, 0)\n",
    "    # NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}\n",
    "    img_mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "    img_std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
    "    img -= img_mean\n",
    "    img /= img_std\n",
    "    return img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa41fe",
   "metadata": {},
   "source": [
    "#### <i>6) Image preprocessing for text recognition</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9bc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess for text recognition\n",
    "def resize_norm_img(img, max_wh_ratio):\n",
    "    \"\"\"\n",
    "    Resize input image for text recognition\n",
    "\n",
    "    Parameters:\n",
    "        img: bounding box image from text detection\n",
    "        max_wh_ratio: value for the resizing for text recognition model\n",
    "    \"\"\"\n",
    "    rec_image_shape = [3, 32, 320]\n",
    "    imgC, imgH, imgW = rec_image_shape\n",
    "    assert imgC == img.shape[2]\n",
    "    character_type = \"ch\"\n",
    "    if character_type == \"ch\":\n",
    "        imgW = int((32 * max_wh_ratio))\n",
    "    h, w = img.shape[:2]\n",
    "    ratio = w / float(h)\n",
    "    if math.ceil(imgH * ratio) > imgW:\n",
    "        resized_w = imgW\n",
    "    else:\n",
    "        resized_w = int(math.ceil(imgH * ratio))\n",
    "    resized_image = cv2.resize(img, (resized_w, imgH))\n",
    "    resized_image = resized_image.astype('float32')\n",
    "    resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
    "    resized_image -= 0.5\n",
    "    resized_image /= 0.5\n",
    "    padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n",
    "    padding_im[:, :, 0:resized_w] = resized_image\n",
    "    return padding_im\n",
    "\n",
    "\n",
    "def prep_for_rec(dt_boxes, frame):\n",
    "    \"\"\"\n",
    "    Preprocessing of the detected bounding boxes for text recognition\n",
    "\n",
    "    Parameters:\n",
    "        dt_boxes: detected bounding boxes from text detection\n",
    "        frame: original input frame\n",
    "    \"\"\"\n",
    "    ori_im = frame.copy()\n",
    "    img_crop_list = []\n",
    "    for bno in range(len(dt_boxes)):\n",
    "        tmp_box = copy.deepcopy(dt_boxes[bno])\n",
    "        img_crop = processing.get_rotate_crop_image(ori_im, tmp_box)\n",
    "        img_crop_list.append(img_crop)\n",
    "\n",
    "    img_num = len(img_crop_list)\n",
    "    # Calculate the aspect ratio of all text bars\n",
    "    width_list = []\n",
    "    for img in img_crop_list:\n",
    "        width_list.append(img.shape[1] / float(img.shape[0]))\n",
    "\n",
    "    # Sorting can speed up the recognition process\n",
    "    indices = np.argsort(np.array(width_list))\n",
    "    return img_crop_list, img_num, indices\n",
    "\n",
    "\n",
    "def batch_text_box(img_crop_list, img_num, indices, beg_img_no, batch_num):\n",
    "    \"\"\"\n",
    "    Batch for text recognition\n",
    "\n",
    "    Parameters:\n",
    "        img_crop_list: processed detected bounding box images\n",
    "        img_num: number of bounding boxes from text detection\n",
    "        indices: sorting for bounding boxes to speed up text recognition\n",
    "        beg_img_no: the beginning number of bounding boxes for each batch of text recognition inference\n",
    "        batch_num: number of images for each batch\n",
    "    \"\"\"\n",
    "    norm_img_batch = []\n",
    "    max_wh_ratio = 0\n",
    "    end_img_no = min(img_num, beg_img_no + batch_num)\n",
    "    for ino in range(beg_img_no, end_img_no):\n",
    "        h, w = img_crop_list[indices[ino]].shape[0:2]\n",
    "        wh_ratio = w * 1.0 / h\n",
    "        max_wh_ratio = max(max_wh_ratio, wh_ratio)\n",
    "    for ino in range(beg_img_no, end_img_no):\n",
    "        norm_img = resize_norm_img(img_crop_list[indices[ino]], max_wh_ratio)\n",
    "        norm_img = norm_img[np.newaxis, :]\n",
    "        norm_img_batch.append(norm_img)\n",
    "\n",
    "    norm_img_batch = np.concatenate(norm_img_batch)\n",
    "    norm_img_batch = norm_img_batch.copy()\n",
    "    return norm_img_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c25984",
   "metadata": {},
   "source": [
    "#### <i>7) Image post-processing for text detection</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocessing image with text detection\n",
    "\n",
    "def post_processing_detection(frame, det_results):\n",
    "    \"\"\"\n",
    "    Postprocess the results from text detection into bounding boxes\n",
    "\n",
    "    Parameters:\n",
    "        frame: input image\n",
    "        det_results: inference results from text detection model\n",
    "    \"\"\"\n",
    "    ori_im = frame.copy()\n",
    "    data = {'image': frame}\n",
    "    data_resize = processing.DetResizeForTest(data)\n",
    "    data_list = []\n",
    "    keep_keys = ['image', 'shape']\n",
    "    for key in keep_keys:\n",
    "        data_list.append(data_resize[key])\n",
    "    img, shape_list = data_list\n",
    "\n",
    "    shape_list = np.expand_dims(shape_list, axis=0)\n",
    "    pred = det_results[0]\n",
    "    if isinstance(pred, paddle.Tensor):\n",
    "        pred = pred.numpy()\n",
    "    segmentation = pred > 0.3\n",
    "\n",
    "    boxes_batch = []\n",
    "    for batch_index in range(pred.shape[0]):\n",
    "        src_h, src_w, ratio_h, ratio_w = shape_list[batch_index]\n",
    "        mask = segmentation[batch_index]\n",
    "        boxes, scores = processing.boxes_from_bitmap(pred[batch_index], mask, src_w, src_h)\n",
    "        boxes_batch.append({'points': boxes})\n",
    "    post_result = boxes_batch\n",
    "    dt_boxes = post_result[0]['points']\n",
    "    dt_boxes = processing.filter_tag_det_res(dt_boxes, ori_im.shape)\n",
    "    return dt_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56f449",
   "metadata": {},
   "source": [
    "### Defining main function to process live images to run the paddleOCR inference\n",
    "\n",
    "<ul>\n",
    "<li>Creating video player to play the live image recording</li>\n",
    "<li>Preparing frames for text detection and recognition process</li>\n",
    "<li>Running AI inference for both text detection and recognition</li>\n",
    "<li>Viisualizing text detection and recognition result</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b943ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_paddle_ocr(source=0, flip=False, use_popup=False, skip_first_frames=0):\n",
    "    \"\"\"\n",
    "    Main function to run the paddleOCR inference:\n",
    "    1. Create a video player to play with target fps (utils.VideoPlayer).\n",
    "    2. Prepare a set of frames for text detection and recognition.\n",
    "    3. Run AI inference for both text detection and recognition.\n",
    "    4. Visualize the results.\n",
    "\n",
    "    Parameters:\n",
    "        source: the webcam number to feed the video stream with primary webcam set to \"0\", or the video path.\n",
    "        flip: to be used by VideoPlayer function for flipping capture image\n",
    "        use_popup: False for showing encoded frames over this notebook, True for creating a popup window.\n",
    "        skip_first_frames: Number of frames to skip at the beginning of the video.\n",
    "    \"\"\"\n",
    "    # create video player to play with target fps\n",
    "    player = None\n",
    "    try:\n",
    "        player = utils.VideoPlayer(source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames)\n",
    "        # Start video capturing\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        while True:\n",
    "            # grab the frame\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # if frame larger than full HD, reduce size to improve the performance\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(src=frame, dsize=None, fx=scale, fy=scale,\n",
    "                                   interpolation=cv2.INTER_AREA)\n",
    "            # preprocess image for text detection\n",
    "            test_image = image_preprocess(frame, 640)\n",
    "\n",
    "            # measure processing time for text detection\n",
    "            start_time = time.time()\n",
    "            # perform the inference step\n",
    "            det_results = det_compiled_model([test_image])[det_output_layer]\n",
    "            stop_time = time.time()\n",
    "\n",
    "            # Postprocessing for Paddle Detection\n",
    "            dt_boxes = post_processing_detection(frame, det_results)\n",
    "\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # use processing times from last 200 frames\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "            processing_time_det = np.mean(processing_times) * 1000\n",
    "\n",
    "            # Preprocess detection results for recognition\n",
    "            dt_boxes = processing.sorted_boxes(dt_boxes)\n",
    "            batch_num = 6\n",
    "            img_crop_list, img_num, indices = prep_for_rec(dt_boxes, frame)\n",
    "\n",
    "            # For storing recognition results, include two parts:\n",
    "            # txts are the recognized text results, scores are the recognition confidence level\n",
    "            rec_res = [['', 0.0]] * img_num\n",
    "            txts = []\n",
    "            scores = []\n",
    "\n",
    "            for beg_img_no in range(0, img_num, batch_num):\n",
    "\n",
    "                # Recognition starts from here\n",
    "                norm_img_batch = batch_text_box(\n",
    "                    img_crop_list, img_num, indices, beg_img_no, batch_num)\n",
    "\n",
    "                # Run inference for text recognition\n",
    "                rec_results = rec_compiled_model([norm_img_batch])[rec_output_layer]\n",
    "\n",
    "                # Postprocessing recognition results\n",
    "                postprocess_op = processing.build_post_process(processing.postprocess_params)\n",
    "                rec_result = postprocess_op(rec_results)\n",
    "                for rno in range(len(rec_result)):\n",
    "                    rec_res[indices[beg_img_no + rno]] = rec_result[rno]\n",
    "                if rec_res:\n",
    "                    txts = [rec_res[i][0] for i in range(len(rec_res))]\n",
    "                    scores = [rec_res[i][1] for i in range(len(rec_res))]\n",
    "\n",
    "            image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            boxes = dt_boxes\n",
    "            # draw text recognition results beside the image\n",
    "            draw_img = processing.draw_ocr_box_txt(\n",
    "                image,\n",
    "                boxes,\n",
    "                txts,\n",
    "                scores,\n",
    "                drop_score=0.5)\n",
    "\n",
    "            # Visualize PaddleOCR results\n",
    "            f_height, f_width = draw_img.shape[:2]\n",
    "            fps = 1000 / processing_time_det\n",
    "            cv2.putText(img=draw_img, text=f\"Inference time: {processing_time_det:.1f}ms ({fps:.1f} FPS)\",\n",
    "                        org=(20, 40),fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=f_width / 1000,\n",
    "                        color=(0, 0, 255), thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            # use this workaround if there is flickering\n",
    "            if use_popup:\n",
    "                draw_img = cv2.cvtColor(draw_img, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imshow(winname=title, mat=draw_img)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # encode numpy array to jpg\n",
    "                draw_img = cv2.cvtColor(draw_img, cv2.COLOR_RGB2BGR)\n",
    "                _, encoded_img = cv2.imencode(ext=\".jpg\", img=draw_img,\n",
    "                                              params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "                # create IPython image\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # display the image in this notebook\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # stop capturing\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddbdc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_paddle_ocr(source=0, flip=False, use_popup=False) # on camera\n",
    "\n",
    "# Test OCR results on video file\n",
    "\n",
    "video_file = \"https://raw.githubusercontent.com/yoyowz/classification/master/images/test.mp4\"\n",
    "run_paddle_ocr(source=video_file, flip=False, use_popup=False, skip_first_frames=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
